{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"HousingJobSpark\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(file_path, file_name):\n",
    "    full_path = os.path.join(file_path, file_name)\n",
    "    return pd.read_csv(full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.getcwd(),'datasets', 'housing')\n",
    "FILE_NAME = \"housing.csv\"\n",
    "housing_data = load_data(DATA_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapOceanProximity(proximity):\n",
    "\n",
    "    if(proximity == \"NEAR BAY\"):\n",
    "        return 1\n",
    "    \n",
    "    elif(proximity == \"INLAND\"):\n",
    "        return 1\n",
    "    \n",
    "    elif(proximity ==\"<1H OCEAN\"):\n",
    "        return 0\n",
    "    \n",
    "    elif(proximity==\"NEAR OCEAN\"):\n",
    "        return 1\n",
    "    \n",
    "    elif(proximity ==\"ISLAND\"):\n",
    "        return 1\n",
    "    \n",
    "    # else:\n",
    "    #     return 0\n",
    "\n",
    "def createLabeledPoint(fields):\n",
    "\n",
    "    longitude = float(fields[0])\n",
    "    latitude = float(fields[1])\n",
    "    housing_median_age = float(fields[2])\n",
    "    total_rooms = float(fields[3])\n",
    "    total_bedrooms = float(fields[4])\n",
    "    population = float(fields[5])\n",
    "    households = float(fields[6])\n",
    "    median_income = float(fields[7])\n",
    "    median_house_value = float(fields[8])\n",
    "    ocean_proximity = mapOceanProximity(fields[9])\n",
    "\n",
    "    return LabeledPoint(ocean_proximity, np.array([longitude, latitude, housing_median_age, total_rooms,\n",
    "                        total_bedrooms, population, households, median_income, median_house_value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_rdd = sc.textFile(os.path.join(DATA_PATH, FILE_NAME))\n",
    "header  = raw_data_rdd.first()\n",
    "raw_data_rdd = raw_data_rdd.filter(lambda x: x != header)\n",
    "\n",
    "raw_data_rdd = raw_data_rdd.map(lambda x: x.split(\",\"))\n",
    "raw_data_rdd = raw_data_rdd.filter(lambda x: x!=\" \")\n",
    "training_data_rdd, test_data_rdd = raw_data_rdd.randomSplit(weights=[8.0,2.0], seed=1)\n",
    "# training_data_rdd, test_data_rdd = training_data_rdd.map(createLabeledPoint), test_data_rdd.map(createLabeledPoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'toDF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-61e952881612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'toDF'"
     ]
    }
   ],
   "source": [
    "df=training_data_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in training_data_rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.array(df.iloc[[0],:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-122.23, 37.88, 41.0, 880.0, 129.0, 322.0, 126.0, 8.3252, 452600.0,\n",
       "       'NEAR BAY'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "randn(d0, d1, ..., dn)\n",
      "\n",
      "Return a sample (or samples) from the \"standard normal\" distribution.\n",
      "\n",
      ".. note::\n",
      "    This is a convenience function for users porting code from Matlab,\n",
      "    and wraps `standard_normal`. That function takes a\n",
      "    tuple to specify the size of the output, which is consistent with\n",
      "    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n",
      "\n",
      ".. note::\n",
      "    New code should use the ``standard_normal`` method of a ``default_rng()``\n",
      "    instance instead; please see the :ref:`random-quick-start`.\n",
      "\n",
      "If positive int_like arguments are provided, `randn` generates an array\n",
      "of shape ``(d0, d1, ..., dn)``, filled\n",
      "with random floats sampled from a univariate \"normal\" (Gaussian)\n",
      "distribution of mean 0 and variance 1. A single float randomly sampled\n",
      "from the distribution is returned if no argument is provided.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "d0, d1, ..., dn : int, optional\n",
      "    The dimensions of the returned array, must be non-negative.\n",
      "    If no argument is given a single Python float is returned.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "Z : ndarray or float\n",
      "    A ``(d0, d1, ..., dn)``-shaped array of floating-point samples from\n",
      "    the standard normal distribution, or a single such float if\n",
      "    no parameters were supplied.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "standard_normal : Similar, but takes a tuple as its argument.\n",
      "normal : Also accepts mu and sigma arguments.\n",
      "Generator.standard_normal: which should be used for new code.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "For random samples from :math:`N(\\mu, \\sigma^2)`, use:\n",
      "\n",
      "``sigma * np.random.randn(...) + mu``\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> np.random.randn()\n",
      "2.1923875335537315  # random\n",
      "\n",
      "Two-by-four array of samples from N(3, 6.25):\n",
      "\n",
      ">>> 3 + 2.5 * np.random.randn(2, 4)\n",
      "array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n",
      "       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "np.random.randn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the spark context and app name \n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"HousingJobSpark\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"this\", \"is\", \"a\", \"string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(lambda x: x+\" test string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this test string', 'is test string', 'a test string', 'string test string']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-100.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('-100.00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
